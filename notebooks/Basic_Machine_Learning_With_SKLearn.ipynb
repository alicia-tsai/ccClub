{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ccClub: Introduction to Machine Learning\n",
    "---\n",
    "## Lab 02: Basic Machine Learning with SK-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "- Hyperparameter & Model Evaluation\n",
    "- Classification: Support Vector Machine\n",
    "- Regression: Least-Square Regression\n",
    "- Clustering: K-means Clustering\n",
    "- Dimensionality Reduction: Principal Components Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "If you have Python2 and Python3 and want to make sure you are installing for Python3, use `pip3 install ...`. For details about installing sklearn, please visit the [documentation](http://scikit-learn.org/stable/install.html).\n",
    "\n",
    ">```python\n",
    "pip install scipy\n",
    "pip install sklearn\n",
    "```\n",
    "\n",
    "If you don't have NumPy, Pandas, Matplotlib or Seaborn, install them as well.\n",
    "\n",
    ">```python\n",
    "pip install numpy\n",
    "pip install pandas\n",
    "pip install matplotlib\n",
    "pip install seaborn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Hyperparameter & Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter\n",
    "\n",
    "For example, when using k-nearest neighbor classification, we have to set the number of neighbors before runing the model. In sklearn's `KNeighborsClassifier(n_neighbors=5)`, `n_neighbor` is the **hyperparameter** that we have to set before training the model.\n",
    "\n",
    "![](img/hyperparameter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "![](img/cross-validation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n"
     ]
    }
   ],
   "source": [
    "# load iris data\n",
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sklearn, `cross_val_score` from `sklearn.model_selection` will return the scores of the estimator for each run of the cross validation.\n",
    "\n",
    "```python\n",
    "step 1: initialize an estimator by calling an appropriate estimator class\n",
    "step 2: do cross validation using `cross_val_score(estimator, X, y, cv=n)`\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation accuracy: [ 0.96666667  1.          0.93333333  0.96666667  1.        ]\n",
      "average accuracy: 0.973333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)  # initialize an estimator\n",
    "scores = cross_val_score(knn, X, y, cv=5)  # do cross-validation\n",
    "\n",
    "print('cross validation accuracy:', scores)\n",
    "print('average accuracy:', scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfitting & Overfitting\n",
    "\n",
    "![](img/underfit_overfit.png)\n",
    "![](img/underfit_overfit_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Classification - Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Regression - Least-Square Regression\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "We can fit a straight line to some real-valued data. You are probably very familiar with this simple linear regression model where $a$ is the **slope** and $b$ is the **intercept**.\n",
    "$$\n",
    "\\hat{y} = ax+b\n",
    "$$\n",
    "\n",
    "What least-square regression does is to find this best line, $\\hat{y} = ax+b$, which minimizes the sum of squared error (or mean squared error).\n",
    "\n",
    "> **Least-square: minimize the sum of squared error (SE) or mean squared error (MSE)**\n",
    "\n",
    "Error means the difference between real answer $y$ and the predicuted value $\\hat{y}$. The sum of squared error is the sum of all $(y - \\hat{y})^2$ for all the data points.\n",
    "\n",
    "> Sum of squared error (SE): $\\sum^{n}_{i=1} (y - \\hat{y})^2$\n",
    "\n",
    "> Mean squared error (MSE): $\\frac{\\sum^{n}_{i=1}(y - \\hat{y})^2}{N}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(100, size=100)\n",
    "y = 5 * x - 3 + np.random.randint(200, size=100)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `LinearRegression` Estimator\n",
    "\n",
    "To fit a linear regression model, we can use sklearn's `LinearRegression()` Estimator from `sklearn.linear_model` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# transform 1D array into 2D (n_sample, n_features), in this example n_features = 1\n",
    "x = x.reshape(len(x), 1)\n",
    "\n",
    "# initialize linear regressin model\n",
    "linear_reg = LinearRegression()\n",
    "\n",
    "# fit data\n",
    "linear_reg.fit(x, y);\n",
    "\n",
    "y_pred = linear_reg.predict(x)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, y_pred, 'r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the coefficients and intercept of the linear regression model by using `.coef_` and `.intercept_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('coefficients:', linear_reg.coef_)\n",
    "print('intercept:', linear_reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Muiltidimensional Linear Regression\n",
    "\n",
    "Now we have a simple linear model, we can expand the same idea into multidimensions, meaning multiple features. Using only one feature is often too simple, we may want to use many features ($x_i$) at the same time. \n",
    "\n",
    "$$\n",
    "\\hat{y} = a_0 + a_1 x_1 + a_2 x_2 + ... a_n x_n\n",
    "$$\n",
    "\n",
    "In a simple linear regression, we are fitting a line to the data. In a 3 dimensions, we are fitting a plane to the data and in higher dimensions, we are fitting a hyper-plane to the data. It is usually very difficult to visualize a hyper-plane in a high-dimension space but we can still see an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(100, size=(100, 3))  # x shape = (100, 3)\n",
    "\n",
    "# coefficients: [0.5, 3, 1.5], intercept: 5\n",
    "y = 5 + 0.5 * x[:, 0] + 3 * x[:, 1] + 1.5 * x[:, 2]  # y shape = (100,)\n",
    "\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize linear regressin model\n",
    "multi_reg = LinearRegression()\n",
    "\n",
    "# fit data\n",
    "multi_reg.fit(x, y);\n",
    "\n",
    "# coeff & intercept\n",
    "print('coefficients:', multi_reg.coef_)\n",
    "print('intercept:', multi_reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression\n",
    "\n",
    "Sometimes a linear relationship may be too simple. If we want to model a non-linear relationship, one common trick is to transform the data according to a basis function and then train a linear regression on the transformed data. This appproach allows the model to learn a much wider range of patterns (relationships) of data while mainting a generally fast performance of a linear model.\n",
    "\n",
    "### Polynomial Features\n",
    "\n",
    "We have our simple linear regression from the previous section.\n",
    "\n",
    "$$\n",
    "\\hat{y} = a_0 + a_1 x\n",
    "$$\n",
    "\n",
    "For example, if we want to fit a parabola to the data instead of a straight line, we can extend our linear regression by combining a second-order polynomials. The model will looks like this, assuming we have one feature $x$ here:\n",
    "\n",
    "$$\n",
    "\\hat{y} = a_0 + a_1 x + a_2 x^2\n",
    "$$\n",
    "\n",
    "We can also combine higher-order polynomials. These are called **polynomial features**.\n",
    "\n",
    "$$\n",
    "\\hat{y} = a_0 + a_1 x + a_2 x^2 + a_3 x^3 + a_4 x^4\n",
    "$$\n",
    "\n",
    "In the example below, we have\n",
    "\n",
    "$$\n",
    "\\hat{y_1} = 5 x -3 \\\\\n",
    "\\hat{y_2} = x + x^2 \\\\\n",
    "\\hat{y_3} = x + x^2 + 0.1 x^3\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomial relationship (degree = 2)\n",
    "x = np.random.randint(50, size=100) - 25\n",
    "y1 = 5 * x - 3 \n",
    "y2 = x + x**2 \n",
    "y3 = x + x**2 + 0.1 * x**3\n",
    "\n",
    "plt.scatter(x, y1, label='degree=1')\n",
    "plt.scatter(x, y2, label='degree=2')\n",
    "plt.scatter(x, y3, label='degree=3')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can have multiple features $x_1, x_2, ... x_n$ and combine their second-order polynomials to construct something more complex. For example, if we have 2 features $x_1, x_2$, we can construct a polynomial regression like this:\n",
    "\n",
    "$$\n",
    "\\hat{y} = a_0 + a_1 x_1 + a_2 x_2 + a_3 x_1x_2 + a_4 x_1^2 + a_5 x_2^2\n",
    "$$\n",
    "\n",
    "### Polynomial Regression Is Still A Linear Model\n",
    "\n",
    "Something surprising here is that a polynomial regression is still a linear model. To observe this, we can create another new variables $z_i$, where\n",
    "\n",
    "$$\n",
    "[z_1, z_2, z_3, z_4, z_5] = [x_1, x_2, x_1x_2, x_1^2, x_2^2]\n",
    "$$\n",
    "\n",
    "And we can re-write our model as\n",
    "\n",
    "$$\n",
    "\\hat{y} = a_0 + a_1 z_1 + a_2 z_2 + a_3 z_3 + a_4 z_4 + a_5 z_5\n",
    "$$\n",
    "\n",
    "We can see that the resulting model is the same class of the multidimensional linear regression. The model is **linear** in $a_n$. The linearity means that the coefficients $a_n$ never multiply or divide each other.\n",
    "\n",
    "By incroporating polynomial features, we have the capability to capture non-linear relationship using only a simple linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "x1 = np.random.randint(50, size=500) - 25\n",
    "x2 = np.random.randint(50, size=500) - 25\n",
    "y = x1 + x2 + x1*x2 + (x1**2) + (x2**2)\n",
    "\n",
    "ax.scatter(x1, x2, y)\n",
    "\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_zlabel('y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `PolynomailFeatures()`\n",
    "\n",
    "We can construct polynomial features directly by using `PolynomialFeatures()` from `sklearn.preprocessing` class.\n",
    "\n",
    "> - input: 2D array\n",
    "> - degree: Integer\n",
    "    - The degree of the polynomial features. \n",
    "    - Default = 2.\n",
    "> - include_bias:\n",
    "    - If True (default), then include a bias column, the feature in which all polynomial powers are zero (i.e. a column of ones - acts as an intercept term in a linear model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct polynomial features with degree = 3\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "x = np.array([[1, 2, 3, 4, 5]]).T  # 2D array\n",
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "print(poly.fit_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# includ_bias = True\n",
    "print(PolynomialFeatures(degree=3, include_bias=True).fit_transform(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline: Automate Machine Learning Workflow\n",
    "\n",
    "Sklearn provdies a handy class `make_pipeline` from `sklearn.pipeline` that can automate the machine learning workflow. It allows various transformation to be chained together. We can also add an estimator (i.e. a model) at the end. Data will flow from the start of the pipleine to the end, and it will be transformed and fed to the next step.\n",
    "\n",
    "> We can use the pipeline object in 2 ways:\n",
    "> 1. `fit` -> `transform`:  if the pipeline ends with a transformer\n",
    "> 2. `fit` -> `predict`: if the pipeline ends with an estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate data\n",
    "x = np.random.randint(50, size=100) - 25  # 1D array\n",
    "y =  x + x**2 + 0.1 * x**3 - np.random.randint(300, size=100)\n",
    "\n",
    "plt.scatter(x, y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "x = x.reshape(len(x), 1)  # convert x into 2D array, x.shape(100, 1)\n",
    "\n",
    "# construct a pipeline\n",
    "poly_reg = make_pipeline(PolynomialFeatures(3), LinearRegression())\n",
    "\n",
    "# ------- equivalent to --------\n",
    "# poly = PolynomialFeatures(3)\n",
    "# x_poly = poly.fit_transform(x)\n",
    "# poly_reg = LinearRegression()\n",
    "# -------------------------------\n",
    "\n",
    "# data will be be transformed into polynomial features and then fed to linear regression model\n",
    "poly_reg.fit(x, y)\n",
    "y_pred = poly_reg.predict(x)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.scatter(x, y_pred, color='r')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
